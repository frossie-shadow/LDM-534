\documentclass[DM,lsstdraft,STS,toc]{lsstdoc}
\usepackage{enumitem}
\input meta.tex

\begin{document}

%set the WP number or product here for the requirements
\def\product{LSST Level 2 System}

\setDocCompact{true}

%Product name first in title
\title[STS for \product]{\product~Software Test Specification}

\author{John D. Swinbank}
\setDocRef{\lsstDocType-\lsstDocNum}
\setDocDate{\vcsdate}
%
% a short abstract
%
\setDocAbstract {
This document describes the detailed test specification for the \product.
}

%
% the title page
%
\maketitle

%
%	Revision history MOST RECENT FIRST
%
\setDocChangeRecord{%
	\addtohist{1}{\vcsdate}{Initial release.}{JDS}
}

\section{Introduction}
\label{sec:intro}

This document specifies the test procedure for the \product.

The \product is the compontent of the LSST system which is responsible for
scientific processing leading to:

\begin{itemize}

  \item{Annual data release production;}
  \item{Periodic (re-) generation of calibration products;}
  \item{Periodic (re-) generation of templates for generating difference
  images, to be consumed in the L1 system;}
  \item{Generating QC metrics based on pipeline execution and post-processing of
  scientific data products.}

\end{itemize}


\subsection{Objectives}
\label{sec:objectives}

This document builds on the description of LSST Data Management's approach to
testing as described in \citeds{LDM-503} to describe the detailed tests that
will be performed on the \product as part of the verification of the DM system.

It identifies test designs, test cases and procedures for the tests, and the
pass/fail criteria for each test. It identifies pass/fail criteria for each
test.

\subsection{Scope}
\label{sec:scope}

This document describes the test procedures for the following components of
the LSST system (as described in \citeds{LDM-148}):

\begin{note}

This list does not include any of the pipeline execution components, which I
take as falling outside this document: should check.

Per mail to KTL, WOM of 2017-06-24, I suggest that we need to add a ``QC
Payloads'' component.

Per the same mail, I suggest that the ``Science Algorithms'' and ``Science
Primitives'' components should not exist.

\end{note}

\begin{itemize}

  \item{Annual Calibration}
  \item{Daily Calibration Update}
  \item{Data Release Production}
  \item{Periodic Calibration}
  \item{Raw Calibration}
  \item{Science Algorithms (partial)}
  \item{Science Primitives (partial)}
  \item{Template Generation}

\end{itemize}

\subsection{Applicable Documents}
\label{sec:docs}

\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
\citeds{LDM-151} & LSST DM Science Pipelines Design \\
\citeds{LDM-294} & LSST DM Organization \& Management \\
\citeds{LDM-502} & The Measurement and Verification of DM Key Performance Metrics \\
\citeds{LDM-503} & LSST DM Test Plan \\
\citeds{LSE-61}  & LSST DM Subsystem Requirements \\
\citeds{LSE-163} & LSST Data Products Definition Document \\
\citeds{LSE-180} & Level 2 Photometric Calibration for the LSST Survey \\
\end{tabular}

\subsection{References\label{sect:references}}
\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

%\subsection{Definitions, acronyms, and abbreviations \label{sect:acronyms}} % include acronyms.tex generated by the acronyms.csh (GaiaTools)
%\input{acronyms}


%----------------------------------------------------
% TASK IDENTIFICATION - APPROACH
%----------------------------------------------------
\section{Approach}
\label{sec:approach}

The major activities to be performed are to:

\begin{itemize}

  \item{Compare the design of the Data Release Production payload as
  implemented to the requirements on the outputs of the DM Subsystem as
  defined in \citeds{LSE-63} and \citeds{LSE-163} to demonstrate that all data
  products required by the scientific community will be delivered by the
  system as built.}

  \item{Ensure that all data products included in the DRP payload design are
  correctly produced and persisted appropriately to the LSST Data Backbone
  when executing a data release production.}

  \item{Compare the design of the Calibration Products payloads as implemented
  to the requirements laid down in \citeds{LSE-63}, the overall design
  described in \citeds{LSE-180} and the inputs of the scientific pipeline
  payloads as described in \citeds{LDM-151}.}

  \item{Ensure that all data products included in the CPP payload design are
  correctly produced and persisted appropriately to the LSST Data Backbone
  and/or Calibration Database when executing a calibration products
  production.}

  \item{Compare the implementation of the Template Generation payloads to the
  inputs required by the Alert Production payload as defined in
  \citeds{LDM-151}.}

  \item{Ensure that all data products required by the L1 system are correctly
  produced and persisted appropriately to the LSST Data Backbone when
  executing a template generation production.}

  \item{Demonstrate that QC metrics are properly calculated and transmitted
  during the execution all L2 production types.}

  \item{Demonstrate that post-processing QC analysis of data products can be
  used to identify and report on failures or anomalies in the processing.}

\end{itemize}

\subsection{Tasks and criteria}
\label{sec:tasks}

The follwing are the major items under test:

\begin{itemize}

  \item{Science payloads capable of generating all LSST Level 2 data
  products;}

  \item{Calibration products payloads, run at a variety of cadences, to
  generate all calibration products required in the generation of LSST Level 1
  and 2 data products;}

  \item{Template generation payloads capable of generating deep teamples
  required for difference imaging in the context of the LSST Level 1 system.}

\end{itemize}

\subsection{Features to be tested}
\label{sec:feat2test}

\begin{itemize}

  \item{Execution of payloads described in \S\ref{sec:tasks};}
  \item{Persistence of all required data products.}

\end{itemize}


\subsection{Features not to be tested}
\label{sec:featnot2test}

This version of the \product test specification addresses only the functional
requirements of the systems under test, as derived from the DM System
Requirements (\citeds{LSE-61}).

A further set of requirements which describe the scientific fidelity of the
output data products are not tested in this version of this test
specification pending flow-down to \citeds{LSE-61}.

The progress of the DM system towards satisfying the scientific requirements
on LSST's data products is tracked by means of a series of Key Performance
Metrics (KPMs) derived from high-level requirements documents (\citeds{LPM-17,
LSE-29, LSE-30}). The system being used to track KPMs and to ensure compliance
with these requirements is described in \citeds{LDM-502}.

\subsection{Pass/fail criteria}
\label{sec:passfail}

The results of all tests will be assessed using the criteria described in
\citeds{LDM-503} \S4.

\subsection{Suspension criteria and resumption requirements}
\label{suspension}

Refer to individual test cases where applicable.

\subsection{Naming convention}

All tests are named according to the pattern \textsc{prod-scope-xx-yy} where:

\begin{description}[font=\normalfont\scshape]

  \item[prod]{The product code, per \citeds{LDM-294}. Relevant entries for this document are:
    \begin{description}[font=\normalfont\scshape,topsep=-1.0ex]
      \item[caldaily]{Daily CP payload}
      \item[cppslow]{Periodic CPP payload}
      \item[cppyear]{Annual CPP payload}
      \item[tmplgen]{Template generation payload}
      \item[cppqc]{CPP QC measurement generators}
      \item[drp]{Annual mini-DRP and DRP payload}
      \item[l2qc]{L2 QC measurement generators}
      \item[cppqc]{CPP QC measurement generators}
    \end{description}
  }
  \item[scope]{The type of test being described:
    \begin{description}[font=\normalfont\scshape,topsep=-1.0ex]
      \item[fun]{concerning functional testing}
      \item[prf]{concerning performance testing}
      \item[int]{concerning integration testing}
      \item[mnt]{concerning maintenance testing}
      \item[acp]{concerning acceptance testing}
      \item[reg]{concerning regression testing}
      \item[ins]{concerning installation testing}
      \item[bck]{concerning backup and restore testing}
      \item[itf]{concerning interface testing}
    \end{description}
  }
  \item[xx]{Test design number (in increments of 10)}
  \item[yy]{Test case number (in increments of 5)}

\end{description}


%--------------------------------------------------
% SPECIFICATION DESIGN OVERVIEW
%--------------------------------------------------
%\section{Specification Design Overview \label{sect:design}}
%Specify refinements of the test approach described in \citeds{LDM-503} if any.
%If not suppress this section .


%
% Here follow test designs
%

\section{Test Specification Design}

\subsection{\textsc{cppslow-ver-00}: Calibration Product Verification}
\label{cppslow-fun-00}

\subsubsection{Objective}

This test design verifies that the calibration products production pipeline as
designed and built meets the overall requirements of the DM system.
Specifically, we verify that:

\begin{itemize}

  \item{The design of the system is such that all calibration products
  required by \citeds{LSE-61} are produced;}

  \item{The code as delivered is accompanied by a suite of unit tests;}

  \item{The code as delivered is accompanied by appropriate documentation;}

  \item{The code complies with all relevant DM coding standards;}

  \item{The code makes use of standard DM interfaces to e.g. the data
  backbone, the logging system, the provenance system;}

  \item{The code is built and tested by the DM continuous integration system.}

\end{itemize}

Note that the tests described in this section apply to all perioically
executed calibration products production payloads, regardless of cadence (the
same codebase will be used for daily updates and annual calibration products
production).

\subsubsection{Approach refinements}

The general approach defined in \citeds{LDM-503} is used. Methods include:

\begin{itemize}

  \item{Document inspection;}
  \item{Code inspection;}
  \item{Review of CI system logs.}

\end{itemize}

\subsubsection{Test case identification}

\begin{longtable} {|p{0.4\textwidth}|p{0.6\textwidth}|}\hline
\textbf{Test Case}  & \textbf{Description} \\\hline
\hyperref[cppslow-ver-00-00]{\textsc{cppslow-ver-00-00}} & CPP design inspection \\\hline
\hyperref[cppslow-ver-00-05]{\textsc{cppslow-ver-00-05}} & CPP code inspection \\\hline
\hyperref[cppslow-ver-00-10]{\textsc{cppslow-ver-00-10}} & CPP testing review \\\hline
\end{longtable}

\section{\textsc{cppslow-fun-10}: Periodic Calibration Product Data Products}
\label{cppslow-fun-10}

\subsection{Objective}

This test design verifies the existence of algorithms for generating of all
periodic calibration products required by the \citeds[DM System
Requirements]{LSE-61}. These include:

\begin{itemize}

  \item{Bad pixel maps;}
  \item{Bias residual images;}
  \item{Crosstalk correction matrices;}
  \item{Illumination correction frames;}
  \item{Monochromatic flat fields;}
  \item{Dark current correction frames;}
  \item{Fringe correction frames.}

\end{itemize}

Note that the tests described in this section apply to all perioically
executed calibration products production payloads, regardless of cadence (the
same codebase will be used for daily updates and annual calibration products
production).

These tests demonstrate the existence of functional algorithms which calculate
the required products; they are not intended to demonstrate the operation of
an integrated calibration products production system.

\subsubsection{Approach refinements}

The general approach defined in \citeds{LDM-503} is used.

The primary test method is to execute the relevant pipeline tasks on some
sample input dataset and to demonstrate that an appropriate output dataset is
produced.

\subsubsection{Test case identification}

\begin{longtable} {|p{0.4\textwidth}|p{0.6\textwidth}|}\hline
\textbf{Test Case}  & \textbf{Description} \\\hline
\hyperref[cppslow-fun-10-00]{\textsc{cppslow-fun-10-00}} & Bad pixel map generation \\\hline
\hyperref[cppslow-fun-10-05]{\textsc{cppslow-fun-10-05}} & Bias residual image generation \\\hline
\hyperref[cppslow-fun-10-10]{\textsc{cppslow-fun-10-10}} & Crosstalk correction matrix generation \\\hline
\hyperref[cppslow-fun-10-15]{\textsc{cppslow-fun-10-15}} & Illumination correction frame generation \\\hline
\hyperref[cppslow-fun-10-20]{\textsc{cppslow-fun-10-20}} & Monochromatic flat field generation \\\hline
\hyperref[cppslow-fun-10-25]{\textsc{cppslow-fun-10-25}} & Dark curent correction frame generation \\\hline
\hyperref[cppslow-fun-10-30]{\textsc{cppslow-fun-10-30}} & Fringe correction frame generation \\\hline
\end{longtable}

%
% Here follow test case specifications
%

\section{Test Case Specification}

\subsection{Preparation}

Before running any test case, the LSST Science Pipelines must be correctly
installed. Follow the procedure described in the
\href{https://pipelines.lsst.io}{Pipelines Documentation}.

\subsection{\textsc{cppslow-ver-00-00}: CPP design inspection}
\label{cppslow-ver-00-00}

\subsubsection{Requirements}

DMS-REQ-0059,DMS-REQ-0060,DMS-REQ-0061,DMS-REQ-0062,DMS-REQ-0063,DMS-REQ-0130,DMS-REQ-0132,DMS-REQ-0282,DMS-REQ-0283.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That the design of the calibration products production pipelines is
  adequate to meet the DM subsystem requirements.}

\end{itemize}

\subsection{Intercase dependencies}

None.

\subsubsection{Procedure}

By reference to \citeds{LDM-151}, the Science Pipelines design document, and
\citeds{LSE-61}, the DM subsystem requirements, demonstrate that:

\begin{itemize}

  \item{The calibration products to be produced by the design outlined in
  \citeds{LDM-151} satisfy all of the DM requirements;}
  \item{All of the calibration products to be produced are required for use by
  either the L1 or L2 science payloads, or have some other identified
  purpose.}

\end{itemize}

\subsection{\textsc{cppslow-ver-00-05}: CPP code inspection}
\label{cppslow-ver-00-05}

\subsubsection{Requirements}

DMS-REQ-0132,DMS-REQ-0158,DMS-REQ-0308.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That the code delivered complies with relevant DM software quality
  standards;}

  \item{That the code is accompanied by appropriate documentation;}

  \item{That the code makes use of appropriate DM interfaces to the rest of
  the system where applicable;}

  \item{That the code is appropriately tested.}
\end{itemize}

\subsection{Intercase dependencies}

None.

\subsubsection{Procedure}

\begin{itemize}

  \item{Check for the existence of a suite of unit test cases accompanying the
  codebase;}

  \item{Check the code to demonstrate that it is written in the standard LSST
  task framework and that it uses only standardized DM interfaces to logging
  and data access (i.e. it does not print directly to screen or perform
  filesystem I/O within the algorithmic code);}

  \item{Check that the code is accompanied by a user manual describing
  procedures for its installation and operation.}

\end{itemize}


\subsection{\textsc{cppslow-ver-00-10}: CPP testing inspection}
\label{cppslow-ver-00-10}

\subsubsection{Requirements}

DMS-REQ-0308.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That all automated test suites associated with the product pass;}
  \item{That there are no unexpected errors or warnings from the build, test
  or installation process.}

\end{itemize}

\subsection{Intercase dependencies}

\hyperref[cppslow-ver-00-05]{\textsc{cppslow-ver-00-05a}}.

\subsubsection{Procedure}

\begin{itemize}

  \item{Check the logs from the LSST CI system which was used to build and
  package the software under test to ensure:

    \begin{itemize}

      \item{Successful execution of the test suite, with no failures and no
      tests being skipped without explanatory documentation.}

      \item{That there were no compiler, test, linter or other warnings
      associated with the software build processing.}

    \end{itemize}
  }

\end{itemize}

\subsection{\textsc{cppslow-fun-10-00}: Bad pixel map generation}
\label{cppslow-fun-10-00}

\subsubsection{Requirements}

DMS-REQ-0059,DMS-REQ-0130.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That a pipeline task (or equivalent tool) exists which generates a list
  of ``bad'' (unusable) pixels for one or more CCDs.}

\end{itemize}

\subsection{Intercase dependencies}

None.

\subsubsection{Input specification}

\begin{note}
Detailed specification of the inputs required to generate an appropriate list of
bad pixels will require further thought \& input from the Calibration Scientist;
this is a work in progress.
\end{note}

\begin{itemize}

  \item{A pre-existing list of known-bad pixels for the CCD under test. This is a Camera Team
  deliverable from sensor acceptance testing.}

  \item{Dark frames corresponding to the CCD under test. (How many? Where from —
  do they need to be on sky, or can we generated them from the test stand?)}

  \item{Flat field frames corresponding to the CCD under test. (How many? Where from —
  do they need to be on sky, or can we generated them from the test stand?)}

  \item{``Pocket pumping'' exposures corresponding to the CCD under test.  (How
  many? Where from — do they need to be on sky, or can we generated them from
  the test stand?)}

\end{itemize}

These products should be available within a Butler repository accessible
through the regular LSST data access framework from the system on which the test
is being run.

\subsubsection{Output specification}

\begin{itemize}

  \item{A list of bad pixels in the CCD under test.}

\end{itemize}

These products should be persisted to a Butler repository accessible through
the regular LSST data access framework from the system on which the test is
being run.

\subsubsection{Procedure}

The task for generating bad pixel masks will be executed from the command line,
with a configuration appropriate for it to fetch required input data from the
input Butler repository.

The resulting bad pixel list will be persisted to the output repository. To
check for correctness, it should be:

\begin{itemize}

  \item{Compared to the initial list of bad pixels provided by the Camera Team;}
  \item{Optionally overplotted on the input data for manual inspection.}

\end{itemize}

\subsection{\textsc{cppslow-fun-10-05}: Bias residual image generation}
\label{cppslow-fun-10-05}

\subsubsection{Requirements}

DMS-REQ-0060,DMS-REQ-0130.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That a pipeline task (or equivalent tool) exists which generates a
  master image which can be used to correct for temporally stable bias structure
  in data from a CCD.}

\end{itemize}

\subsection{Intercase dependencies}

None.

\subsubsection{Input specification}

\begin{note}
Detailed specification of the inputs required will require further thought \&
input from the Calibration Scientist; this is a work in progress.
\end{note}

\begin{itemize}

  \item{Multiple (how many?) zero-second exposures of the CCD under test.}

\end{itemize}

These products should be available within a Butler repository accessible
through the regular LSST data access framework from the system on which the test
is being run.

\subsubsection{Output specification}

\begin{itemize}

  \item{A master bias residual image.}

\end{itemize}

These products should be persisted to a Butler repository accessible through
the regular LSST data access framework from the system on which the test is
being run.

\subsubsection{Procedure}

The task for generating the master bias residual image will be executed from the
command line, with a configuration appropriate for it to fetch required input
data from the input Butler repository.

The resulting master bias will be persisted to the output repository. It should
be retrieved from the output repository using the Butler and checked to ensure
it contains physically plausible values (TBD by the Calibration Scientist).

\subsection{\textsc{cppslow-fun-10-10}: Crosstalk correction matrix generation}
\label{cppslow-fun-10-10}

\subsubsection{Requirements}

DMS-REQ-0061,DMS-REQ-0130.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That a pipeline task (or equivalent tool) exists which generates a
  matrix describing the fraction of the signal detected in any given amplifier
  on each sensor in the focal plane appears in any other amplifier.}

\end{itemize}

Note that crosstalk is sensitive to the details of the camera configuration
(circuit board locations, cable flex, etc), and so the final values of the
crosstalk correction matrix cannot be measured until the camera is in situ on
the mountain (and even then they may continue to evolve, necessitating periodic
re-measurement). However, this test verifies the operation of the algorithm for
generating the matrix, not the values used in operation, so this test does not
need to be run with the camera in its final configuration.

\subsection{Intercase dependencies}

None.

\subsubsection{Input specification}

\begin{note}
Detailed specification of the inputs required will require further thought \&
input from the Calibration Scientist; this is a work in progress.
\end{note}

\begin{itemize}

  \item{Dithered Colliated Beam Projector (CBP) observations with the full
  camera or a representative subset thereof.}

\end{itemize}

These products should be available within a Butler repository accessible
through the regular LSST data access framework from the system on which the test
is being run.

\subsubsection{Output specification}

\begin{itemize}

  \item{A crosstalk correction matrix.}

\end{itemize}

These products should be persisted to a Butler repository accessible through
the regular LSST data access framework from the system on which the test is
being run.

\subsubsection{Procedure}

The task for generating the crosstalk correction matrix will be executed from the
command line, with a configuration appropriate for it to fetch required input
data from the input Butler repository.

The resulting crosstalk correction matrix will be persisted to the output
repository. It should be retrieved from the output repository using the Butler
and checked to ensure it contains physically plausible values (TBD by the
Calibration Scientist).

\subsection{\textsc{cppslow-fun-10-15}: Illumination correction frame generation}
\label{cppslow-fun-10-15}

\subsubsection{Requirements}

DMS-REQ-0062,DMS-REQ-0130.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That a pipeline task (or equivalent tool) exists which generates an
  image that corrects for the non-uniform illumination of the flat field
  screen.}

\end{itemize}

\subsection{Intercase dependencies}

None.

\subsubsection{Input specification}

\begin{note}
Detailed specification of the inputs required will require further thought \&
input from the Calibration Scientist; this is a work in progress.
\end{note}

\begin{itemize}

  \item{Collimated Beam Projector (CBP) images as specified in \citeds{LDM-151}
  \S4.2.10.}

\end{itemize}

These products should be available within a Butler repository accessible
through the regular LSST data access framework from the system on which the test
is being run.

\subsubsection{Output specification}

\begin{itemize}

  \item{An illumination correction image.}

\end{itemize}

These products should be persisted to a Butler repository accessible through
the regular LSST data access framework from the system on which the test is
being run.

\subsubsection{Procedure}

The task for generating the illumination correction frame will be executed from the
command line, with a configuration appropriate for it to fetch required input
data from the input Butler repository.

The resulting illumination correction image will be persisted to the output
repository. It should be retrieved from the output repository using the Butler
and checked to ensure it contains physically plausible values (TBD by the
Calibration Scientist; ultimately, per \citeds{LSE-61}, it will be verified by
application to operational data).


\subsection{\textsc{cppslow-fun-10-20}: Monochromatic flat field generation}
\label{cppslow-fun-10-20}

\subsubsection{Requirements}

DMS-REQ-0063,DMS-REQ-0130.

\subsubsection{Test items}

This test will check:

\begin{itemize}

  \item{That a pipeline task (or equivalent tool) exists which generates an
  set of master pure monochromatic flat field images .}

\end{itemize}

\subsection{Intercase dependencies}
bias-, overscan-, and dark-corrected

DMS-REQ-0060,DMS-REQ-0062,DMS-REQ-0282.

\subsubsection{Input specification}

\begin{note}
Detailed specification of the inputs required will require further thought \&
input from the Calibration Scientist; this is a work in progress.
\end{note}

\begin{itemize}

  \item{Monochromatic flat field images;}
  \item{Collimated Beam Projector (CBP) images as specified in \citeds{LDM-151}
  \S4.2.10.}

\end{itemize}

These products should be available within a Butler repository accessible
through the regular LSST data access framework from the system on which the test
is being run.

\subsubsection{Output specification}

\begin{itemize}

  \item{A monochromatic flat field data cube.}

\end{itemize}

These products should be persisted to a Butler repository accessible through
the regular LSST data access framework from the system on which the test is
being run.

\subsubsection{Procedure}

Tasks for assembling, bias correcting and dark correcting the monochromatic
flat field images will be executed from the command line, and the results
persisted to a data repository. These serve as inputs to the monochromatic flat
field data cube production.

The task for generating the monochromatic flat field data cube will be executed
from the command line, and the results persisted to a further data repository.

The Butler will be used to retrieve the flat field data cube from the output
repository, and the contents checked to ensure they are physically plausible
(values TBD by the Calibration Scientist.)


%% NON-TESTING VERIFICATION METHODS TEST CASES
%\subsection{Test Case \product-VER-XX-YY \label{sect:testcaseid}}
%
%\subsubsection{Requirements \label{sect:reqs}}
%Specify the requirements that fulfill the test case, comma-separated and ending with '.'\\
%e.g. CU3-IDT-XM-FUN-30,CU3-IDT-ASD-FUN-20.\\
%Note that the format is essential for the script that creates the traceability matrices works fine.
%
%\subsubsection{Test items \label{sect:tcitems}}
%Identify and briefly describe the items and features to be exercised by this test case (e.g.:
%\begin{itemize}
%\item  demonstrate that an
%specific document is written or that it contains the information required
%\item or...
%\item demonstrate that a parameter is set to a specific value
%\item or...
%\item the code is written in python3
%\end{itemize}
%
%\subsubsection{Intercase dependencies \label{sect:interface_dependencies}}
%List the identifiers of the test cases that must be executed prior to this test case. Summarize the nature of the dependencies.
%
%\subsubsection{Procedure \label{sect:procedures}}
%If the procedure is shared by various test cases it would be recommended to separate the procedure description from the test
%case definition. Thus, only a reference to the procedure identification shall be provided. The complete description of
%the test procedure shall be given otherwise. \\
%
%Describe any special constraints on the test procedures that execute this test case. These constraints may involve special set
%up, operator intervention, output determination procedures, and special wrap up.

%
%
%% NON-TESTING VERIFICATION METHODS TEST DESIGN
%\section{\product-VER-XX \label{sect:designid_verification}}
%
%\subsection{Objective \label{sect:designobj}}
%This test design includes all the verification methods different to the dynamic testing, needed for the
%demonstration of the fulfillment of specific requirements of the DMSR.\\
%This includes  the requirements that do not need the execution of the software systems, but a careful document or code inspection, review
%or analysis.
%
%\subsection{Features to be tested \label{sect:totest}}
%Identify the test items and describe the features and combinations of features that are the object of this design
%specification. E.g.:\\
%\begin{itemize}
%\item Demonstrate that the CU has produced an specific document (e.g. the Software Development Plan)
%\item Demonstrate the behavior or value of an specific parameter (e.g. "As a minimum the PPN parameter shall be included in
%the global model"
%\item Static analysis using tools such as Findbugs or checkstyle, etc.
%\item etc
%\end{itemize}
%The features to be tested depends on the content of the Software Requirements Document. Therefore the above list is just
%an example.
%
%\subsection{Approach refinements \label{sect:approach}}
%Specify the verification methods to be used to verify the features describe in the section above. These methods are
%described in the DM SVTP \citeds{LDM-503}. E.g.:
%\begin{itemize}
%\item Code inspection
%\item Document inspection
%\item Review
%\item Static analysis
%\item etc
%\end{itemize}
%
%\subsection{Test case identification \label{sect:testcaselist}}
%List the identifier and a brief description of each test case associated with this design.
%We need to agree on the identifiers here .. perhaps they come out of MD ?
%
%\begin{longtable} {|p{0.4\textwidth}|p{0.6\textwidth}|}\hline
%{\bf Test Case}  & {\bf Description}  \\\hline
%\product-VER-XX-10 & design inspection \\\hline
%\product-VER-XX-15 & Code inspection \\\hline
%\product-VER-XX-20 & Testing review \\\hline
%etc & \\\hline
%\end{longtable}
%The number and scope of the test cases depend on the requirements to be verified. The list provide above is only an example.
%
%\subsection{Feature pass/fail criteria}
%Specify the specific criteria for this design to be used to determine whether the feature or feature combination has passed or failed.
%
%% NON-TESTING VERIFICATION METHODS TEST CASES
%\subsection{Test Case \product-VER-XX-YY \label{sect:testcaseid}}
%
%\subsubsection{Requirements \label{sect:reqs}}
%Specify the requirements that fulfill the test case, comma-separated and ending with '.'\\
%e.g. CU3-IDT-XM-FUN-30,CU3-IDT-ASD-FUN-20.\\
%Note that the format is essential for the script that creates the traceability matrices works fine.
%
%\subsubsection{Test items \label{sect:tcitems}}
%Identify and briefly describe the items and features to be exercised by this test case (e.g.:
%\begin{itemize}
%\item  demonstrate that an
%specific document is written or that it contains the information required
%\item or...
%\item demonstrate that a parameter is set to a specific value
%\item or...
%\item the code is written in python3
%\end{itemize}
%
%\subsubsection{Intercase dependencies \label{sect:interface_dependencies}}
%List the identifiers of the test cases that must be executed prior to this test case. Summarize the nature of the dependencies.
%
%\subsubsection{Procedure \label{sect:procedures}}
%If the procedure is shared by various test cases it would be recommended to separate the procedure description from the test
%case definition. Thus, only a reference to the procedure identification shall be provided. The complete description of
%the test procedure shall be given otherwise. \\
%
%Describe any special constraints on the test procedures that execute this test case. These constraints may involve special set
%up, operator intervention, output determination procedures, and special wrap up.
%
%
%\section{\product-SCOPE-XX \label{sect:designid}}
%
%\subsection{Objective}
%Specify the objective of this test design.
%
%\subsection{Features to be tested}
%Identify the test items and describe the features and combinations of features that are the object of this design
%specification. Other features may be exercised, but need not be identified.\\
%For each feature or feature combination, a reference to its associated requirements should be included.
%
%\subsection{Approach refinements}
%Specify refinements to the approach described in the test plan. Include specific test techniques to be used.
%The method on analysis test results should be identified.\\
%Specify the results of any analysis that provides a rationale for test case selection.\\
%Summarize the common attributes of any test cases. This may include input constraints that must be true for every input
%in the set of associated test cases, any shared environmental needs, any shared special procedural requirements and any
%shared case dependencies.
%
%\subsection{Test case identification}
%List the identifier and a brief description of each test case associated with this design.
%
%\begin{longtable} {|p{0.4\textwidth}|p{0.6\textwidth}|}\hline
%{\bf Test Case}  & {\bf Description}  \\\hline
%\product-SCOPE-XX-YY &
%Description of the test case \\\hline
%\end{longtable}
%
%\subsection{Feature pass/fail criteria}
%Specify the specific criteria for this design to be used to determine whether the feature or feature combination has passed or failed.
%
%
%%\newpage
%
%%--------------------------------------------------
%% TEST CASE SPECIFICATION
%%--------------------------------------------------
%%\subsection{Test Case Specification \label{sect:testcases}}
%%Define the test cases identified by a test design specification. For each identify test case specify:
%
%%\newpage
%
%\subsection{Test Case \product-SCOPE-XX-YY}
%
%\subsubsection{Requirements}
%Specify the requirements that fulfill the test cases, comma-separated and ending with '.'\\
%e.g. CU3-IDT-XM-FUN-30,CU3-IDT-ASD-FUN-20.\\
%Note that the format is essential for the script that creates the traceability matrices works fine.
%
%\subsubsection{Test items}
%Identify and briefly describe the items and features to be exercised by this test case.\\
%For each item, consider supplying references to the following test item documentation: requirements specification, design
%specification, user guide, operations guide, installation guide, etc.
%
%\subsubsection{Input specification \label{sect:tcinput}}
%Specify each input required to execute the test case. Some of the inputs will be specified by value (with tolerances where
%appropriate), while others will be specified by name.\\
%Identify all appropriate databases, files, terminal messages, memory resident areas, and values passed by the operating system.\\
%Specify all required relationships between inputs (e.g. timing).
%
%\subsubsection{Output specification \label{sect:tcoutput}}
%Specify all the outputs and features (e.g. response time) required of the test items.
%
%\subsubsection{Environmental needs \label{sect:tcenvironment}}
%\paragraph{Hardware \label{sect:tchw}}
%Specify the characteristics and configurations of the hardware required to execute this test case.
%\paragraph{Software \label{sect:tcsw}}
%Specify the system and application software required to execute this test case. This may include system software such as operating
%systems, compilers, simulators, and test tools.
%\paragraph{Other \label{sect:tcother}}
%Any other special requirements such as unique facility needs or specially trained personnel.
%
%\subsubsection{Intercase dependencies}
%List the identifiers of the test cases that must be executed prior to this test case. Summarize the nature of the dependencies.
%
%\subsubsection{Procedure}
%If the procedure is shared by various test cases it would be recommended to separate the procedure description from the test
%case definition. Thus, only a reference to the procedure identification shall be provided. The complete description of
%the test procedure shall be given otherwise. \\
%
%Describe any special constraints on the test procedures that execute this test case. These constraints may involve special set
%up, operator intervention, output determination procedures, and special wrap up.
%
%\newpage
%
%%-----------------------------------------
%% PROCEDURES
%%-----------------------------------------
%\section{Test Procedure Specification \label{proc_spec}}
%
%This section could be removed if the test cases specification describe their specific procedure, i.e. the procedure is included
%in the test case definition.
%
%\subsection{Introduction \label{sect:peroc_intro}}
%The purpose is to specify the steps for executing a set of test cases or, more generally, the steps used to analyze a software in order
%to evaluate a set of features.\\
%For every test procedure:
%
%\subsection{[PROCEDURE IDENTIFIER] \label{sect:procedureid}}
%
%\subsubsection{Purpose \label{sect:proc_purpose}}
%Describe the purpose of this procedure. Provide the reference of the test cases that are executed by the procedure.
%
%\subsubsection{Special requirements \label{sect:proc_reqs}}
%Identify any special requirements that are necessary for the execution of this procedure. These may include prerequisite procedures,
%special skills requirements and special environmental requirements.
%
%\subsubsection{Procedure steps \label{sect:proc_steps}}
%Describe every step of each procedure execution. Include the following steps as applicable:
%\paragraph{Log \label{sect:proc_log}}
%Describe any special methods or format for logging the results of test execution, the incidents observed, and any other events
%pertinent to the test.
%\paragraph{Set up \label{sect:proc_setup}}
%Describe the sequence of actions necessary to set up the procedure execution.
%\paragraph{Start \label{sect:proc_start}}
%Describe the actions necessary to begin the procedure execution.
%\paragraph{Proceed \label{sect:proc_proceed}}
%Describe the actions necessary during the procedure execution.
%\paragraph{Measure \label{sect:proc_measure}}
%Describe how the test measurements is made.
%\paragraph{Shut down \label{sect:proc_shutdown}}
%Describe the action necessary to suspend testing when interruption is forced by unscheduled events.
%\paragraph{Restart \label{sect:proc_restart}}
%Identify any procedural restart points and describe the actions necessary to restart the procedure at each of these points.
%\paragraph{Wrap up \label{sect:proc_wrapup}}
%Describe the actions necessary to terminate testing.
%\paragraph{Contingencies \label{sect:proc_contingencies}}
%Describe the actions necessary to deal with anomalous events that may occur during execution.
%
%
%\newpage
%
%\appendix
%\section{TRACEABILITY \label{sect:traceability}}
%The backward and forward traceability (i.e. requirements trace to a test and a test to a requirement, respectively) shall be provided in order to
%ascertain the correlations between the test cases and the requirements they fulfill.\\
%
%\begin{longtable}{|p{0.44\textwidth}|p{0.2\textwidth}|p{0.3\textwidth}|
%}\hline
%{\bf SRS Requirement} & {\bf Verification Method} & {\bf Partially/Completely Verified}
%\\\hline
%\end{longtable} \normalsize
%
%\textbf{Ruby scripts for this are available if th document is exactly like this
% template - but we may be doing this in Magic Draw .. we need to decide on
% that ..}
\end{document}
